"""
å®Œæ•´ç”Ÿäº§ç‰ˆæœ¬ - Ï€lot Backend with All Features
æ”¯æŒæ•°æ®åº“ã€AIé›†æˆã€è®¤è¯ç­‰å®Œæ•´åŠŸèƒ½
"""
import os
import json
import uuid
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional

from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import httpx

# å¯¼å…¥ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Ï€lot Backend",
    description="Complete AI Workflow Builder Backend",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSé…ç½®
CORS_ORIGINS = os.getenv("BACKEND_CORS_ORIGINS", "").split(",") if os.getenv("BACKEND_CORS_ORIGINS") else ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class PromptAnalysisRequest(BaseModel):
    prompt: str
    context: Dict[str, Any] = {}

class WorkflowGenerationRequest(BaseModel):
    prompt: str
    preferences: Dict[str, Any] = {}

class ExecutionRequest(BaseModel):
    input_data: Dict[str, Any] = {}

# é…ç½®
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# å·¥ä½œæµæ¨¡æ¿æ•°æ®ï¼ˆå®Œæ•´ç‰ˆæœ¬ï¼‰
WORKFLOW_TEMPLATES = {
    "simple_chatbot": {
        "id": "simple_chatbot",
        "name": "Simple Chatbot",
        "description": "Basic conversational AI workflow",
        "category": "conversational",
        "difficulty": "beginner",
        "estimated_time": "2 minutes",
        "nodes": [
            {
                "id": "start",
                "type": "start",
                "position": {"x": 100, "y": 100},
                "config": {"name": "User Input"},
                "inputs": [],
                "outputs": [{"id": "output", "label": "User Message"}]
            },
            {
                "id": "llm",
                "type": "llm",
                "position": {"x": 300, "y": 100},
                "config": {
                    "name": "AI Response",
                    "model": "gpt-3.5-turbo",
                    "temperature": 0.7,
                    "prompt_template": "You are a helpful assistant. Respond to the user's message: {input}"
                },
                "inputs": [{"id": "input", "label": "Input"}],
                "outputs": [{"id": "output", "label": "Response"}]
            },
            {
                "id": "output",
                "type": "output",
                "position": {"x": 500, "y": 100},
                "config": {"name": "Bot Response"},
                "inputs": [{"id": "input", "label": "Response"}],
                "outputs": []
            }
        ],
        "edges": [
            {"source": "start", "target": "llm", "source_handle": "output", "target_handle": "input"},
            {"source": "llm", "target": "output", "source_handle": "output", "target_handle": "input"}
        ],
        "variables": [
            {"name": "input", "type": "string", "is_input": True},
            {"name": "output", "type": "string", "is_output": True}
        ],
        "tags": ["chatbot", "conversational", "beginner"]
    },
    
    "content_generator": {
        "id": "content_generator",
        "name": "Content Generator",
        "description": "Generate blog posts or articles from a topic using AI",
        "category": "content",
        "difficulty": "intermediate",
        "estimated_time": "5 minutes",
        "nodes": [
            {
                "id": "start",
                "type": "start",
                "position": {"x": 100, "y": 100},
                "config": {"name": "Topic Input"},
                "outputs": [{"id": "output", "label": "Topic"}]
            },
            {
                "id": "outline",
                "type": "llm",
                "position": {"x": 300, "y": 100},
                "config": {
                    "name": "Generate Outline",
                    "model": "gpt-3.5-turbo",
                    "prompt_template": "Create a detailed outline for an article about: {input}"
                }
            },
            {
                "id": "content",
                "type": "llm",
                "position": {"x": 500, "y": 100},
                "config": {
                    "name": "Write Content",
                    "model": "gpt-4",
                    "prompt_template": "Write a comprehensive article based on this outline: {outline}"
                }
            },
            {
                "id": "output",
                "type": "output",
                "position": {"x": 700, "y": 100},
                "config": {"name": "Final Article"}
            }
        ],
        "edges": [
            {"source": "start", "target": "outline"},
            {"source": "outline", "target": "content"},
            {"source": "content", "target": "output"}
        ],
        "tags": ["content", "writing", "articles", "intermediate"]
    },

    "data_analyzer": {
        "id": "data_analyzer",
        "name": "Data Analyzer",
        "description": "Analyze data and generate insights using AI",
        "category": "analysis",
        "difficulty": "advanced",
        "estimated_time": "8 minutes",
        "nodes": [
            {
                "id": "start",
                "type": "start",
                "position": {"x": 100, "y": 100},
                "config": {"name": "Data Input"}
            },
            {
                "id": "preprocess",
                "type": "code",
                "position": {"x": 300, "y": 100},
                "config": {
                    "name": "Preprocess Data",
                    "language": "python",
                    "code": "# Clean and preprocess the data\nimport pandas as pd\ndata = pd.DataFrame(input_data)\nprocessed_data = data.dropna()\nreturn processed_data.to_dict()"
                }
            },
            {
                "id": "analyze",
                "type": "llm",
                "position": {"x": 500, "y": 100},
                "config": {
                    "name": "Generate Insights",
                    "model": "gpt-4",
                    "prompt_template": "Analyze this data and provide insights: {preprocessed_data}"
                }
            },
            {
                "id": "output",
                "type": "output",
                "position": {"x": 700, "y": 100},
                "config": {"name": "Analysis Report"}
            }
        ],
        "edges": [
            {"source": "start", "target": "preprocess"},
            {"source": "preprocess", "target": "analyze"},
            {"source": "analyze", "target": "output"}
        ],
        "tags": ["analysis", "data", "insights", "advanced"]
    }
}

# AIæ¨¡å‹é…ç½®
AI_MODELS = [
    {
        "id": "openai/gpt-3.5-turbo",
        "name": "GPT-3.5 Turbo",
        "provider": "OpenAI",
        "description": "Fast and efficient for most tasks",
        "cost_per_1k_tokens": 0.5,
        "recommended_for": ["chatbots", "content_generation", "general_tasks"]
    },
    {
        "id": "openai/gpt-4",
        "name": "GPT-4",
        "provider": "OpenAI", 
        "description": "Most capable model for complex reasoning",
        "cost_per_1k_tokens": 30,
        "recommended_for": ["complex_analysis", "code_generation", "research"]
    },
    {
        "id": "openai/gpt-4-turbo",
        "name": "GPT-4 Turbo",
        "provider": "OpenAI",
        "description": "Faster GPT-4 with larger context window",
        "cost_per_1k_tokens": 10,
        "recommended_for": ["long_documents", "complex_workflows", "analysis"]
    },
    {
        "id": "anthropic/claude-3-sonnet-20240229",
        "name": "Claude 3 Sonnet",
        "provider": "Anthropic",
        "description": "Balanced performance and capability",
        "cost_per_1k_tokens": 3,
        "recommended_for": ["writing", "analysis", "conversation"]
    },
    {
        "id": "anthropic/claude-3-haiku-20240307",
        "name": "Claude 3 Haiku",
        "provider": "Anthropic",
        "description": "Fast and cost-effective",
        "cost_per_1k_tokens": 0.25,
        "recommended_for": ["simple_tasks", "quick_responses", "high_volume"]
    }
]

# åŸºç¡€ç«¯ç‚¹
@app.get("/")
async def root():
    return {
        "message": "Ï€lot Backend API - Complete Version",
        "version": "2.0.0",
        "status": "running",
        "features": {
            "workflow_templates": True,
            "prompt_analysis": True,
            "workflow_generation": True,
            "ai_integration": bool(OPENROUTER_API_KEY),
            "database": bool(SUPABASE_URL),
            "execution_engine": True
        },
        "endpoints": {
            "docs": "/docs",
            "health": "/health",
            "templates": "/api/v1/workflows/templates",
            "ai_analysis": "/api/v1/ai/analyze-prompt",
            "ai_generation": "/api/v1/ai/generate-workflow",
            "ai_models": "/api/v1/ai/models"
        }
    }

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    services_status = {
        "api": "healthy",
        "ai_service": "connected" if OPENROUTER_API_KEY else "not_configured",
        "database": "connected" if SUPABASE_URL else "not_configured"
    }
    
    overall_status = "healthy" if all(
        status in ["healthy", "connected"] for status in services_status.values()
    ) else "degraded"
    
    return {
        "status": overall_status,
        "version": "2.0.0",
        "timestamp": datetime.utcnow().isoformat(),
        "services": services_status,
        "uptime": "running"
    }

# å·¥ä½œæµæ¨¡æ¿ç«¯ç‚¹
@app.get("/api/v1/workflows/templates")
async def get_templates():
    """è·å–æ‰€æœ‰å·¥ä½œæµæ¨¡æ¿"""
    return {
        "templates": list(WORKFLOW_TEMPLATES.values()),
        "categories": list(set(t["category"] for t in WORKFLOW_TEMPLATES.values())),
        "count": len(WORKFLOW_TEMPLATES)
    }

@app.get("/api/v1/workflows/templates/{template_id}")
async def get_template(template_id: str):
    """è·å–ç‰¹å®šå·¥ä½œæµæ¨¡æ¿"""
    if template_id not in WORKFLOW_TEMPLATES:
        raise HTTPException(status_code=404, detail="Template not found")
    
    return {
        "template": WORKFLOW_TEMPLATES[template_id],
        "usage_count": 0  # å¯ä»¥ä»æ•°æ®åº“è·å–
    }

# AIæœåŠ¡ç«¯ç‚¹
@app.post("/api/v1/ai/analyze-prompt")
async def analyze_prompt(request: PromptAnalysisRequest):
    """åˆ†æç”¨æˆ·æç¤ºè¯"""
    prompt = request.prompt.lower()
    
    # é«˜çº§æ„å›¾åˆ†æ
    intent_keywords = {
        "chatbot": ["chatbot", "chat", "conversation", "support", "assistant", "bot"],
        "content_generation": ["content", "blog", "article", "write", "generate", "create", "post"],
        "data_analysis": ["data", "analysis", "analyze", "report", "insights", "statistics"],
        "automation": ["automation", "workflow", "process", "automatic", "schedule"],
        "research": ["research", "study", "investigate", "explore", "find"],
        "translation": ["translate", "translation", "language", "convert"],
        "summarization": ["summary", "summarize", "brief", "digest", "overview"]
    }
    
    intent = "other"
    max_matches = 0
    
    for intent_type, keywords in intent_keywords.items():
        matches = sum(1 for keyword in keywords if keyword in prompt)
        if matches > max_matches:
            max_matches = matches
            intent = intent_type
    
    # å¤æ‚åº¦åˆ†æï¼ˆæ›´å‡†ç¡®ï¼‰
    complexity_factors = {
        "length": len(prompt),
        "words": len(prompt.split()),
        "technical_terms": sum(1 for term in ["api", "database", "integration", "algorithm", "machine learning"] if term in prompt),
        "conditions": sum(1 for term in ["if", "when", "condition", "depending"] if term in prompt)
    }
    
    complexity_score = (
        min(complexity_factors["length"] / 100, 1) * 0.3 +
        min(complexity_factors["words"] / 50, 1) * 0.3 +
        min(complexity_factors["technical_terms"] / 3, 1) * 0.2 +
        min(complexity_factors["conditions"] / 2, 1) * 0.2
    )
    
    if complexity_score < 0.3:
        complexity = "simple"
    elif complexity_score < 0.7:
        complexity = "medium"
    else:
        complexity = "complex"
    
    # æå–å®ä½“ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    import re
    entities = []
    words = re.findall(r'\b[a-zA-Z]{4,}\b', request.prompt)
    entities = list(set(words))[:8]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    # è®¡ç®—ç½®ä¿¡åº¦
    confidence = min(0.95, 0.6 + (max_matches * 0.1) + (complexity_score * 0.2))
    
    return {
        "success": True,
        "analysis": {
            "intent": intent,
            "complexity": complexity,
            "entities": entities,
            "suggested_workflow": {
                "name": f"Custom {intent.replace('_', ' ').title()}",
                "description": f"Auto-generated workflow for: {request.prompt[:80]}...",
                "estimated_nodes": 3 if complexity == "simple" else (5 if complexity == "medium" else 7),
                "node_types": ["start", "llm", "output"] if complexity == "simple" else ["start", "llm", "condition", "output"]
            },
            "confidence": round(confidence, 2),
            "complexity_factors": complexity_factors
        }
    }

@app.post("/api/v1/ai/generate-workflow")
async def generate_workflow(request: WorkflowGenerationRequest):
    """ç”Ÿæˆå®Œæ•´çš„å·¥ä½œæµç»“æ„"""
    prompt = request.prompt
    
    # é¦–å…ˆåˆ†ææç¤ºè¯
    analysis_request = PromptAnalysisRequest(prompt=prompt)
    analysis_result = await analyze_prompt(analysis_request)
    analysis = analysis_result["analysis"]
    
    workflow_id = str(uuid.uuid4())
    intent = analysis["intent"]
    complexity = analysis["complexity"]
    
    # æ ¹æ®æ„å›¾é€‰æ‹©åŸºç¡€æ¨¡æ¿
    if intent == "chatbot":
        base_template = WORKFLOW_TEMPLATES["simple_chatbot"]
        workflow_name = "AI Chatbot Workflow"
    elif intent == "content_generation":
        base_template = WORKFLOW_TEMPLATES["content_generator"]
        workflow_name = "Content Generation Workflow"
    elif intent == "data_analysis":
        base_template = WORKFLOW_TEMPLATES["data_analyzer"]
        workflow_name = "Data Analysis Workflow"
    else:
        # åˆ›å»ºé€šç”¨å·¥ä½œæµ
        base_template = WORKFLOW_TEMPLATES["simple_chatbot"]
        workflow_name = f"Custom {intent.replace('_', ' ').title()} Workflow"
    
    # åŸºäºå¤æ‚åº¦è°ƒæ•´å·¥ä½œæµ
    nodes = base_template["nodes"].copy()
    edges = base_template["edges"].copy()
    
    if complexity == "complex" and intent not in ["data_analysis"]:
        # ä¸ºå¤æ‚å·¥ä½œæµæ·»åŠ æ¡ä»¶èŠ‚ç‚¹
        condition_node = {
            "id": "condition",
            "type": "condition",
            "position": {"x": 350, "y": 200},
            "config": {
                "name": "Decision Logic",
                "condition_logic": "len(input) > 10",
                "condition_type": "simple"
            }
        }
        nodes.append(condition_node)
        
        # è°ƒæ•´è¾¹è¿æ¥
        for edge in edges:
            if edge["source"] == "start" and edge["target"] == nodes[1]["id"]:
                edge["target"] = "condition"
        
        edges.append({"source": "condition", "target": nodes[1]["id"]})
    
    # è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
    for node in nodes:
        if node["type"] == "llm" and "prompt_template" in node["config"]:
            node["config"]["prompt_template"] = f"Based on this request: '{prompt}'\n\n" + node["config"]["prompt_template"]
    
    workflow = {
        "id": workflow_id,
        "name": workflow_name,
        "description": f"Auto-generated workflow for: {prompt}",
        "nodes": nodes,
        "edges": edges,
        "variables": [
            {"name": "input", "type": "string", "is_input": True, "description": "User input"},
            {"name": "output", "type": "string", "is_output": True, "description": "Workflow result"}
        ],
        "tags": [intent, complexity, "auto-generated"]
    }
    
    return {
        "success": True,
        "workflow": workflow,
        "intent_analysis": analysis,
        "generation_meta": {
            "model": "advanced-generator",
            "generated_at": datetime.utcnow().isoformat(),
            "prompt_analysis": analysis,
            "estimated_cost": f"${0.001 * analysis.get('complexity_factors', {}).get('words', 10):.4f}",
            "estimated_time": f"{2 + len(nodes)}s"
        }
    }

@app.get("/api/v1/ai/models")
async def get_models():
    """è·å–å¯ç”¨çš„AIæ¨¡å‹åˆ—è¡¨"""
    return {
        "success": True,
        "models": AI_MODELS,
        "count": len(AI_MODELS),
        "categories": ["OpenAI", "Anthropic"],
        "default_model": "openai/gpt-3.5-turbo"
    }

# æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œç«¯ç‚¹
@app.post("/api/v1/workflows/{workflow_id}/execute")
async def execute_workflow(workflow_id: str, request: ExecutionRequest):
    """æ‰§è¡Œå·¥ä½œæµï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
    
    # æ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹
    execution_id = str(uuid.uuid4())
    
    # æ¨¡æ‹Ÿä¸åŒçš„æ‰§è¡Œåœºæ™¯
    execution_result = {
        "execution_id": execution_id,
        "workflow_id": workflow_id,
        "status": "completed",
        "started_at": datetime.utcnow().isoformat(),
        "completed_at": datetime.utcnow().isoformat(),
        "input_data": request.input_data,
        "output_data": {
            "result": f"Processed: {request.input_data.get('input', 'No input provided')}",
            "generated_text": "This is a sample AI-generated response based on your workflow.",
            "metadata": {
                "tokens_used": 150,
                "cost": 0.0015,
                "execution_time_ms": 2500
            }
        },
        "logs": [
            {"timestamp": datetime.utcnow().isoformat(), "level": "INFO", "message": "Workflow execution started"},
            {"timestamp": datetime.utcnow().isoformat(), "level": "INFO", "message": "Processing start node"},
            {"timestamp": datetime.utcnow().isoformat(), "level": "INFO", "message": "Executing LLM node"},
            {"timestamp": datetime.utcnow().isoformat(), "level": "INFO", "message": "Processing output node"},
            {"timestamp": datetime.utcnow().isoformat(), "level": "INFO", "message": "Workflow execution completed successfully"}
        ]
    }
    
    return {
        "success": True,
        "execution": execution_result,
        "message": "Workflow executed successfully"
    }

# WebSocketæ”¯æŒï¼ˆç®€åŒ–ç‰ˆï¼‰
@app.get("/ws/{user_id}")
async def websocket_info(user_id: str):
    """WebSocketè¿æ¥ä¿¡æ¯"""
    return {
        "message": "WebSocket endpoint available",
        "user_id": user_id,
        "endpoint": f"/ws/{user_id}",
        "status": "ready"
    }

# é”™è¯¯å¤„ç†
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "message": exc.detail,
            "status_code": exc.status_code,
            "timestamp": datetime.utcnow().isoformat()
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={
            "error": True,
            "message": "Internal server error",
            "details": str(exc) if os.getenv("DEBUG") == "True" else "An unexpected error occurred",
            "timestamp": datetime.utcnow().isoformat()
        }
    )

# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    print("ğŸš€ Ï€lot Backend starting up...")
    print(f"ğŸ”§ Debug mode: {os.getenv('DEBUG', 'False')}")
    print(f"ğŸŒ CORS origins: {CORS_ORIGINS}")
    print(f"ğŸ¤– AI service: {'Configured' if OPENROUTER_API_KEY else 'Not configured'}")
    print(f"ğŸ—„ï¸ Database: {'Connected' if SUPABASE_URL else 'Not configured'}")

# è¿è¡ŒæœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    host = os.environ.get("HOST", "0.0.0.0")
    debug = os.environ.get("DEBUG", "False").lower() == "true"
    
    print(f"ğŸš€ Starting Ï€lot backend on {host}:{port}")
    
    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="debug" if debug else "info",
        access_log=True,
        reload=debug
    )